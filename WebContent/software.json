[
  {
    "name": "Metric Program Synthesis",
    "blurb": "We present a new domain-agnostic synthesis technique for generating programs from input-output examples. Our method, called metric program synthesis, relaxes the well-known observational equivalence idea (used widely in bottom-up enumerative synthesis) into a weaker notion of observational similarity, with the goal of reducing the search space that the synthesizer needs to explore. <cite>FeserDS22metric</cite>",
    "authors": "Feser, John, Isil Dillig, and Armando Solar-Lezama",
    "link": "https://github.com/jfeser/symetric"
  },

  {
    "name": "Web question answering with neurosymbolic program synthesis",
    "blurb":"In this paper, we propose a new technique based on program synthesis for extracting information from webpages. Given a natural language query and a few labeled webpages, our method synthesizes a program that can be used to extract similar types of information from other unlabeled webpages <cite>webq</cite>",
    "authors": "Chen, Qiaochu, Aaron Lamoreaux, Xinyu Wang, Greg Durrett, Osbert Bastani, and Isil Dillig",
    "link": "https://github.com/utopia-group/WebQA"
  },

  {
    "title": "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
    "blurb":" We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems <cite>EllisWNSMHCST21</cite>",
    "authors": "Ellis, Kevin, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum",
    "link": "https://github.com/ellisk42/ec"
  },

  {
    "name": "Neural program generation modulo static analysis",
    "blurb":"ate-of-the-art neural models of source code tend to be evaluated on the generation of individual expressions and lines of code, and commonly fail on long-horizon tasks such as the generation of entire method bodies. We propose to address this deficiency using weak supervision from a static program analyzer. <cite>neurostatistic</cite>",
    "authors": "Mukherjee, Rohan, Yeming Wen, Dipak Chaudhari, Thomas Reps, Swarat Chaudhuri, and Christopher Jermaine",
    "link": "https://github.com/rohanmukh/nsg"
  },

  {
    "name": "Safe Neurosymbolic Learning with Differentiable Symbolic Execution",
    "blurb":"We study the problem of learning worst-case-safe parameters for programs that use neural networks as well as symbolic, human-written code. Such neurosymbolic programs arise in many safety-critical domains. <cite>safe</cite>",
    "authors": "Yang, Chenxi, and Swarat Chaudhuri",
    "link": "https://github.com/chenxi-yang/DSE"
  },

  {
    "name": "Program Synthesis Guided Reinforcement Learning for Partially Observed Environments",
    "blurb":"A key challenge for reinforcement learning is solving long-horizon planning problems. Recent work has leveraged programs to guide reinforcement learning in these settings. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. <cite>yang2021program</cite>",
    "authors": "Yang, Yichen, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, and Martin Rinard",
    "link": "https://github.com/yycdavid/program-synthesis-guided-RL "
  },

  {
    "name": "Neurosymbolic transformers for multi-agent communication",
    "blurb":"We study the problem of inferring communication structures that can solve cooperative multi-agent planning problems while minimizing the amount of communication. We quantify the amount of communication as the maximum degree of the communication graph; this metric captures settings where agents have limited bandwidth. <cite> inala2020neurosymbolic </cite>",
    "authors": "Inala, Jeevana Priya, Yichen Yang, James Paulos, Yewen Pu, Osbert Bastani, Vijay Kumar, Martin Rinard, and Armando Solar-Lezama",
    "link": "https://github.com/jinala/multi-agent-neurosym-transformers"
  },

  {
    "name": "Pragmatic Code Autocomplete",
    "blurb":"In this work, we aim to make programming languages more concise by allowing programmers to utilize a controlled level of ambiguity. <cite>poesia2021pragmatic</cite>",
    "authors": "Poesia, Gabriel, and Noah Goodman",
    "link": "https://github.com/gpoesia/magicomplete"
  },

  {
    "name": "DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates",
    "blurb":"CPU simulators are useful tools for modeling CPU execution behavior. However, they suffer from inaccuracies due to the cost and complexity of setting their fine-grained parameters, such as the latencies of individual instructions. This complexity arises from the expertise required to design benchmarks and measurement frameworks that can precisely measure the values of parameters at such fine granularity. <cite>renda2020difftune</cite>",
    "authors": "Renda, Alex, Yishen Chen, Charith Mendis, and Michael Carbin",
    "link": "https://github.com/ithemal/DiffTune "
  },

  {
    "name": "Unsupervised Learning of Neurosymbolic Encoders",
    "blurb":"We present a framework for the unsupervised learning of neurosymbolic encoders, which are encoders obtained by composing neural networks with symbolic programs from a domain-specific language. Our framework naturally incorporates symbolic expert knowledge into the learning process, which leads to more interpretable and factorized latent representations compared to fully neural encoders. <cite>zhan2021unsupervised</cite>",
    "authors": "Eric Zhan, Jennifer J. Sun, Ann Kennedy, Yisong Yue and Swarat Chaudhuri",
    "link": "https://github.com/ezhan94/neurosymbolic-encoders"
  },

  {
    "name": "Learning Differentiable Programs with Admissible Neural Heuristics",
    "blurb":"We study the problem of learning differentiable functions expressed as programs in a domain-specific language. Such programmatic models can offer benefits such as composability and interpretability; however, learning them requires optimizing over a combinatorial space of program architectures. We frame this optimization problem as a search in a weighted graph whose paths encode top-down derivations of program syntax. <cite>near</cite>",
    "authors": "Ameesh Shah, Eric Zhan, Jennifer Sun, Abhinav Verma, Yisong Yue, Swarat Chaudhuri",
    "link": "https://github.com/trishullab/near"
  },

  {
    "name": " Few-Shot Image Classification: Just Use a Library of Pre-Trained Feature Extractors and a Simple Classifier ",
    "blurb":" Recent papers have suggested that transfer learning can outperform sophisticated meta-learning methods for few-shot image classification. We take this hypothesis to its logical conclusion, and suggest the use of an ensemble of high-quality, pre-trained feature extractors for few-shot image classification. <cite>Chowdhury_2021_ICCV</cite>",
    "authors": "Arkabandhu Chowdhury, Mingchao Jiang, Swarat Chaudhuri, Chris Jermaine",
    "link": "https://github.com/arjish/PreTrainedFullLibrary_FewShot"
  },

  {
    "name": "Independent se (3)-equivariant models for end-to-end rigid protein docking",
    "blurb":"Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding <cite>ganea2021independent</cite>",
    "authors": "Ganea, Octavian-Eugen, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause",
    "link": "https://github.com/octavian-ganea/equidock_public"
  },

  {
    "name": "Torsional geometric generation of molecular 3d conformer ensembles",
    "blurb":"<cite>jing2022torsional</cite>",
    "authors": "Ganea, Octavian, Lagnajit Pattanaik, Connor Coley, Regina Barzilay, Klavs Jensen, William Green, and Tommi Jaakkola",
    "link": "https://github.com/PattanaikL/GeoMol"
  },

  {
    "name": "EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction",
    "blurb":"Predicting how a drug-like molecule binds to a specific protein target is a core problem in drug discovery.EquiBind, an SE(3)-equivariant geometric deep learning model performing direct-shot prediction of both i) the receptor binding location (blind docking) and ii) the ligand’s bound pose and orientation. <cite>stark2022equibind</cite>",
    "authors": "Stärk, Hannes, Octavian Ganea, Lagnajit Pattanaik, Regina Barzilay, and Tommi Jaakkola",
    "link": "https://github.com/HannesStark/EquiBind"
  },

  {
    "name": "Torsional Diffusion for Molecular Conformer Generation",
    "blurb":"Molecular conformer generation is a fundamental task in computational chemistry. Several machine learning approaches have been developed, but none have outperformed state-of-the-art cheminformatics methods. We propose torsional diffusion, a novel diffusion framework that operates on the space of torsion angles via a diffusion process on the hypertorus and an extrinsic-to-intrinsic score model. <cite>jing2022torsional</cite>",
    "authors": "Jing, Bowen, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola",
    "link": "https://github.com/gcorso/torsional-diffusion"
  },

  {
    "name": "Crystal diffusion variational autoencoder for periodic material generation",
    "blurb":"Generating the periodic structure of stable materials is a long-standing challenge for the material design community.  We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability  <cite>xie2021crystal</cite>",
    "authors": "Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, Tommi Jaakkola",
    "link": "https://github.com/txie-93/cdvae "
  },

  {
    "name": "Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem",
    "blurb":"<cite>trippe2022diffusion</cite>",
    "authors": "Trippe, Brian L., Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, and Tommi Jaakkola",
    "link": "https://openreview.net/attachment?id=6TxBxqNME1Y&name=supplementary_material"
  },

  {
    "name": "Synthesizing theories of human language with Bayesian program induction",
    "blurb":"Automated, data-driven construction and evaluation of scientific models and theories is a long-standing challenge in artificial intelligence. We present a framework for algorithmically synthesizing models of a basic part of human language: morpho-phonology, the system that builds word forms from sounds <cite>Ellis22Linguistics</cite>",
    "authors": "Ellis, Kevin, Adam Albright, Armando Solar-Lezama, Joshua B. Tenenbaum, and Timothy J. O’Donnell",
    "link": "https://github.com/ellisk42/bpl_phonology"
  },

  {
    "name": "Task programming: Learning data efficient behavior representations",
    "blurb":"we present TREBA: a method to learn annotation-sample efficient trajectory embedding for behavior analysis, based on multi-task self-supervised learning.  <cite>sun2021task</cite>",
    "authors": "Jennifer J. Sun, Ann Kennedy, Eric Zhan, David J. Anderson, Yisong Yue, Pietro Perona",
    "link": "https://github.com/neuroethology/TREBA"
  },

  {
    "name": "Contrastive Reinforcement Learning of Symbolic Reasoning Domains",
    "blurb":"Abstract symbolic reasoning, as required in domains such as mathematics and logic, is a key component of human intelligence. Solvers for these domains have important applications, especially to computer-assisted education. But learning to solve symbolic problems is challenging for machine learning algorithms. Existing models either learn from human solutions or use hand-engineered features, making them expensive to apply in new domains. <cite>poesia2021contrastive</cite>",
    "authors": "Gabriel Poesia, WenXin Dong, Noah Goodman",
    "link": "https://github.com/gpoesia/socratic-tutor"
  },

  {
    "name": "Leveraging Language to Learn Program Abstractions and Search Heuristics",
    "blurb":"We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. <cite>wong2021leveraging</cite>",
    "authors": "Wong, Catherine, Kevin M. Ellis, Joshua Tenenbaum, and Jacob Andreas",
    "link": "https://github.com/ellisk42/ec/tree/icml_2021_supplement"
  },

  {
    "name": " Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis",
    "blurb":" We present AutoSWAP: a framework for automatically synthesizing data-efficient task-level LFs. The key to our approach is to efficiently represent expert knowledge in a reusable domain-specific language. <cite>tseng2022automatic</cite>",
    "authors": "Tseng, Albert, Jennifer J. Sun, and Yisong Yue",
    "link": "https://github.com/tsengalb99/AutoSWAP"
  },

  {
    "name": " Self-Supervised Keypoint Discovery in Behavioral Videos ",
    "blurb":" We propose a method for learning the posture and structure of agents from unlabelled behavioral videos. Starting from the observation that behaving agents are generally the main sources of movement in behavioral videos, our method, Behavioral Keypoint Discovery (B-KinD), uses an encoder-decoder architecture with a geometric bottleneck to reconstruct the spatiotemporal difference between video frames. <cite>sun2022self</cite>",
    "authors": "Sun, Jennifer J., Serim Ryou, Roni H. Goldshmid, Brandon Weissbourd, John O. Dabiri, David J. Anderson, Ann Kennedy, Yisong Yue, and Pietro Perona.",
    "link": "https://github.com/neuroethology/BKinD"
  },

  {
    "name": "The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions",
    "blurb":"Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay <cite>sun2021multi</cite>",
    "authors": "Jennifer J. Sun, Tomomi Karigo, Dipam Chakraborty, Sharada P. Mohanty, Benjamin Wild, Quan Sun, Chen Chen, David J. Anderson, Pietro Perona, Yisong Yue, Ann Kennedy",
    "link": "https://data.caltech.edu/records/s0vdx-0k302 "
  },

  {
    "name": "The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior",
    "blurb":"We introducing a large-scale, multi-agent trajectory dataset from real-world behavioral neuroscience experiments that covers a range of behavior analysis tasks. Our dataset consists of trajectory data from common model organisms, with 9.6 million frames of mouse data and 4.4 million frames of fly data, in a variety of experimental settings, such as different strains, lengths of interaction, and optogenetic stimulation. <cite>sun2022mabe22</cite>",
    "authors": "Jennifer J. Sun, Andrew Ulmer, Dipam Chakraborty, Brian Geuther, Edward Hayes, Heng Jia, Vivek Kumar, Zachary Partridge, Alice Robie, Catherine E. Schretter, Chao Sun, Keith Sheppard, Param Uttarwar, Pietro Perona, Yisong Yue, Kristin Branson, Ann Kennedy",
    "link": "https://data.caltech.edu/records/8kdn3-95j37"
  }
]
