<html>

<!--
    To add a paper:

    1. Add its bibtex entry to the file `biblio.bib`
    2. Go to the notion notes for the paper and export as html. Unzip the exported directory to the folder `reading-group-notes`.
    2. Add a <project> frame by copying one of the others. Update the description, citation, and local notion link.
-->

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Neurosymbolic Reading Group</title>

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.13.216/pdf.min.js"></script>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" src="/scripts/library.js"></script>
    <script type="text/javascript" src="overrides.js"></script>
    <link href="style.css" rel="stylesheet" type="text/css">
</head>

<body onresize="resizing()">


    <div class="header">
        <h1 style="color: rgb(165, 42, 42);">Understanding the World Through Code</h1>
        <h2 style="color: 'black';">Funded through the NSF Expeditions in Computing Program</h2>
        <div class="logos">
            <img title='mit' src='images/mit.png' height='100%' />
            <img title='ut' src='images/ut.png' height='100%' />
            <img title='caltech' src='images/caltech.png' height='100%' />
            <img title='rice' src='images/rice.png' height='100%' />
            <img title='penn' src='images/penn.png' height='100%' />
            <img title='stanford' src='images/stanford.png' height='100%' />
        </div>
        <div id='navicon.div' class='navicon'>
            <img id='navicon' src="images/burger.rest.png" onmouseenter="iconEnter(this, 'burger')"
                onmouseleave="iconLeave(this, 'burger')" onclick='toggleNav()' title="Navigation" width="34pt"></img>
        </div>
    </div>


    <div id="navigator" class="sidenav">
    </div>
    <script type="text/javascript">
        loadNavBar();
    </script>

    <div class="content">

        <h1>Neurosymbolic Reading Group</h1>

        <p>
            We are organizing a reading group to discuss papers related to the project. The goal of the reading group is
            to help us understand the state of the art in the field.

            Currently, we plan to meet on Wednesday at 5-6PM EST, but we prioritize meeting authors, so this sometimes
            can change the meeting time. We are still planning the schedule for the rest of the semester, but the next
            few meetings are:

        <ul>
            <!-- <li>2023-09-06: 5:00-6:00 PM EST. Omar Costilla-Reyes will present a talk</li>
            <li>2023-09-13: 5:00-6:00 PM EST. TBD</li> -->
            <!-- 11/1 : TBD -->
            <li>2023-11-01: 5:00-6:00 PM EST. TBD</li>
            <li>2023-11-08: 5:00-6:00 PM EST. Daniel Fried will present TBD</li>
            <li>2023-11-15: 5:00-6:00 PM EST. Ruocheng and Linlu will present
                <a href="https://arxiv.org/abs/2309.05660">Hypothesis Search: Inductive Reasoning with Language
                    Models</a>
                and
                <a href="https://arxiv.org/abs/2310.08559">Phenomenal Yet Puzzling: Testing Inductive Reasoning
                    Capabilities of Language Models with Hypothesis Refinement</a>
            </li>

        </ul>
        <!-- <li>2023-04-17: 5:00-6:00 PM EST, Programmatically Grounded, Compositionally Generalizable Robotic
                Manipulation (with Jiayuan Mao and Ren Wang)</li> -->

        Please contact Alex Gu (gua+nrg@mit.edu) for information on how to join the reading group. If you would like to
        present,
        please fill out <a
            href="https://docs.google.com/forms/d/e/1FAIpQLSfEUex12WLCk2lFFeP2_x4PWG2VvnG1GFWhx-eJXRTnwqGs4w/viewform">
            this form</a>.
        </p>

        <h1>Previously Covered Papers and Notes.</h1>

        <div class="project">
            <h1>Neurosymbolic Programming in Mental Health</h1>
    
            <p><i>Read on November 1, 2023</i></p>
            Speakers: Morgan Talbot (MIT) and Omar Costilla-Reyes (MIT) 
    
            <br>Abstract: Our overburdened mental healthcare system has a compelling need for new evidence-based approaches to diagnosis, prognosis, and treatment. Machine learning in mental health research offers vast potential for future clinical applications, but comes with significant challenges. Promising research directions include digital phenotyping, a process of leveraging data from personal digital devices to predict mental states, network analysis to quantify relations among symptoms or other factors, and counterfactual analysis to elucidate causal relationships that drive mental health and to identify appropriate treatments for individual patients. Key obstacles in the field include limited availability of large-scale datasets, noise and missingness in longitudinal datasets from patients' smartphones and symptom self-reports, and the extraordinary complexity of the many inter-related processes in patients' lives that affect their mental health. We will explore a range of research questions and the challenges we have encountered in addressing them, with a goal of advancing towards the application of advanced techniques such as program synthesis and neurosymbolic programming in mental health research.
    
            Slides can be found <a
                href=https://docs.google.com/presentation/d/1ouu69Et3VkCcUxzpkOO3MnayWs_BiC1KENqQ5-ZXhkw/edit?usp=sharing">here</a>.
        </div>

        <div class="project">
            <h1>Code llama: Open foundation models for code</h1>

            <p><i>Read on October 25, 2023</i></p>

            We discussed the paper Code llama: Open foundation models for code
            <cite>roziere2023code</cite> with author Baptiste Rozière.
            The paper a set of open source foundation models for code.
            Slides can be found <a
                href="https://drive.google.com/file/d/1QH5UXSumMSYbXFcU49gesB0HZUUszCbD/view">here</a>.
        </div>

        <div class="project">
            <h1>Differentiable Tree Operations Promote Compositional Generalization</h1>

            <p><i>Read on October 18, 2023</i></p>

            We discussed the paper Differentiable Tree Operations Promote Compositional Generalization
            <cite>soulos2023differentiable</cite> with author Paul Soulos.
            The paper describes a technique for representing trees as vectors and performing differentiable operations
            on them, rather than flattening them into sequences.
            Slides can be found <a
                href="https://docs.google.com/presentation/d/1kovir_-5_z09-P71TqEPVETBXJpeEJlF/edit?usp=drive_link&ouid=115848284222225341054&rtpof=true&sd=true">here</a>.
        </div>

        <div class="project">
            <h1>WizardCoder: Empowering Code Large Language Models with Evol-Instruct</h1>

            <p><i>Read on October 11, 2023</i></p>

            We discussed the paper WizardCoder: Empowering Code Large Language Models with Evol-Instruct
            <cite>luo2023wizardcoder</cite> with the author Ziyang Luo.
            The paper describes a technique that empowers Code LLMs with complex instruction fine-tuning, by adapting
            the Evol-Instruct method to the domain of code. Slides can be found <a
                href="https://drive.google.com/file/d/1aEC6Cr4ZWgdpJz7cqJIig347bOiBxtis/view">here</a>.
        </div>

        <div class="project">
            <h1>Evidence of Meaning in Language Models Trained on Programs</h1>

            <p><i>Read on October 4, 2023</i></p>

            We discussed the paper Evidence of Meaning in Language Models Trained on Programs
            <cite>jin2023evidence</cite> with the author Charles Jin.
            The paper describes a causal framework for understanding the intermediate states of a language model as
            it is executed on data, and how this can be correlated with semantic features of the program. Slides can be
            found <a href="https://drive.google.com/file/d/1jYr3PC076Zj_B_zYlhpEcth8ggGRUhVx/view">here</a>.
        </div>

        <div class="project">
            <h1>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through
                Counterfactual Tasks</h1>

            <p><i>Read on September 13, 2023</i></p>

            We discussed the paper Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models
            Through Counterfactual Tasks
            <cite>wu2023reasoning</cite> with the author Zhaofeng Wu.
            The paper describes an analysis where "counterfactual" tasks are used instead of familiar ones, to
            determine how much the model is generalizing. Slides can be found <a
                href="https://drive.google.com/file/d/1MK8j5pk39i2KttXFS_FOBpgY4fQYt0Aq/view">here</a>.
        </div>

        <div class="project">
            <h1>Neurosym Introduction</h1>

            <p><i>Read on September 6, 2023</i></p>

            Omar Costilla-Reyes gave a talk kicking off the reading group for the fall. Slides can be found <a
                href="https://drive.google.com/file/d/1Ze_pLrtqHWgAjzGt-tBr3uy0Bnidn5g4/view">here</a>.
        </div>

        <div class="project">
            <h1>Language Model Query Language</h1>

            <p><i>Read on May 15, 2023</i></p>

            We discussed the paper Prompting Is Programming: A Query Language for
            Large Language Models <cite>beurer2022prompting</cite> with the authors Luca Beurer-Kellner and
            Marc Fisher. The paper describes a new programming language that is used to perform structured
            inference with LLMs. Slides can be found <a
                href="https://drive.google.com/file/d/1zhMmUwSWPzYl4Uuwn3B1fB_Yehv6ZkOB/view">here</a>.
        </div>

        <div class="project">
            <h1>Interpretability Discussion</h1>

            <p><i>Read on May 8, 2023</i></p>

            Omar Costilla Reyes and Atharva Seghal lead a discussion on interpretability. Slides can be
            found <a
                href="https://docs.google.com/presentation/d/1YFYUvX1949ZlIzGOy-CjBXE_nE1upckUI_WMmxPn95c/edit">here</a>.
        </div>

        <div class="project">
            <h1>Inducing causal structure for interpretable neural networks</h1>

            <p><i>Read on April 24, 2023</i></p>

            We discussed the paper Inducing causal structure for interpretable neural networks
            <cite>geiger2022inducing</cite> with the authors Atticus Geiger and Zhengxuan Wu.
            The paper describes a process by which do-notation-style interchange interventions
            are performed on a neural network to align certain neurons with certain states in
            a known causal model of the process being modeled.
        </div>

        <div class="project">
            <h1>Programmatically Grounded, Compositionally Generalizable Robotic
                Manipulation</h1>

            <p><i>Read on April 17, 2023</i></p>

            We discussed the paper Programmatically Grounded, Compositionally Generalizable Robotic
            Manipulation<cite>wangprogrammatically</cite> with the authors Jiayuan Mao and Ren Wang.
            The paper describes a modular approach to vision-to-actuation robotics pipelines that uses
            the syntactic and semantic structure of language instructions.
        </div>

        <div class="project">
            <h1>Scallop: A Language for Neurosymbolic Programming</h1>

            <p><i>Read on April 10, 2023</i></p>

            We discussed the Scallop paper<cite>li2023scallop</cite> which is upcoming at PLDI 2023 with authors Ziyang
            Li and Jiani Huang.
            The paper describes a prolog-like language for neurosymbolic programming that allows a user to write a
            logical program as the
            second half of a pipeline that takes as input the outputs of a neural network. The logical program is then
            treated as manipulating
            the distributions over the outputs of the neural network, and thus is differentiable, allowing for
            end-to-end training of the
            pipeline.

            Notes can be found <a
                href="reading-group-notes/Scallop A Language for Neurosymbolic Programming b524edb34ce1449dbb09e4c3af4fd084.html">here</a>.
        </div>


        <div class="project">
            <h1>From word models to world models: Translating from natural language to the probabilistic language of
                thought</h1>

            <p><i>Read on April 3, 2023</i></p>

            Gabe Grand gave a presentation on an upcoming position paper on the topic of solving reasoning tasks by
            having a model learn to translate from natural language to the probabilistic language of thought.
            Specifically, an LLM is prompted with a world model and then used to translate conditions, queries,
            and even new definitions into Church statements, allowing the use of sampling procedures to infer
            the answers to queries. This approach outperforms LLMs trying to directly reason without the use
            of the symbolic component.
        </div>

        <div class="project">
            <h1>Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions and LEVER: Learning to
                Verify Language-to-Code Generation with Execution</h1>

            <p><i>Read on March 13, 2023</i></p>

            We discussed these papers<cite>nilearning</cite> <cite>ni2023lever</cite> with the author Ansong Ni.
            These papers describe an approach to learn mathematical solutions from a dataset of prefix programs
            and an algorithm to improve the output of code generators via a separate
            reranking network trained to look at their execution results. A video of the presentation can
            be found <a
                href="https://mit.zoom.us/rec/play/jmIKan0Rl9Z8secv_FzJ8aRr9jw-PNsf6yww0GH27TqdYvNhrDQKxCOULFnHd9Y-F_zCRzoNEElzByBg.fR9UEOBwkegagn-y?continueMode=true&_x_zm_rtaid=uGdQ0vFoSuC_2qCdG-fMtA.1680209472187.b0d3d23fae60142e3f58ab021d449dee&_x_zm_rhtaid=614">
                here
            </a>
        </div>


        <div class="project">
            <h1>Productivity Assessment of Neural Code Completion</h1>

            <p><i>Read on March 6, 2023</i></p>

            We discussed this paper<cite>ziegler2022productivity</cite> with the author Shawn Simister.
            This paper discusses aspects of the deployment of copilot as well as metrics of success
            in improving practical productivity. Notes can be found
            <a
                href="reading-group-notes/Productivity Assessment of Neural Code Completion  1316e8b77df7492eb000c6146ceee1f8.html">
                here
            </a>.
        </div>


        <div class="project">
            <h1>Looped Transformers as Programmable Computers</h1>

            <p><i>Read on February 27, 2023</i></p>

            We discussed this paper<cite>giannou2023looped</cite> with the author Dimitris Papailiopoulos.
            This paper demonstrates an algorithm to convert programs into transformers, highlighting the extent
            to which this can be accomplished with a small number of transformer layers. Notes will be posted
            shortly.
        </div>


        <div class="project">
            <h1>Planning with Large Language Models for Code Generation</h1>

            <p><i>Read on February 13, 2023</i></p>

            We discussed this paper<cite>zhang2022planning</cite> with the authors Shun Zhang and Zhenfang Chen.
            This paper proposes an algorithm to more effectively sample from code transformers in which prefixes that
            lead to better programs are weighted more heavily. Our notes can be found
            <a
                href="reading-group-notes/Planning with Large Language Models for Code Gener 3eaeb5a0f3454cec83cc5330ad6436b8.html">
                here</a>.
        </div>


        <div class="project">
            <h1>Parsel: A Unified Natural Language Framework for Algorithmic Reasoning</h1>

            <p><i>Read on February 6, 2023</i></p>

            We discussed Parsel<cite>zelikman2022parsel</cite> with the authors. This paper proposes a framework
            enabling automatic
            implementation and validation of complex algorithms with code LLMs, using hierarchical function descriptions
            as an intermediate
            language. It is able to outperform Codex and AlphaCode on the APPS dataset. Our notes can be found
            <a
                href="reading-group-notes/Parsel A (De-)compositional Framework for Algorith 73d91ea15e3848bf90a9f5da0ad95639.html">
                here</a>.
        </div>

        <div class="project">
            <h1>Binding language models in symbolic languages</h1>

            <p><i>Read on January 31, 2023</i></p>

            We discussed the new Binder<cite>cheng2022binding</cite> technique with the authors.
            This paper proposes Binder, a training-free neural-symbolic framework that maps the task input to a program,
            which (1) allows
            binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python)
            to extend its
            grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the
            underlying
            model called by the API during execution, and (3) requires only a few in-context exemplar annotations.

            Our notes can
            be found
            <a
                href="reading-group-notes/Binding Language Models in Symbolic Languages a63033a5fe754b09bed7437917d06601.html">here</a>.
        </div>


        <div class="project">
            <h1>Learning Differentiable Programs with Admissible Neural Heuristics</h1>

            <p><i>Read on January 24, 2023</i></p>

            We discussed NEAR<cite>shah2020learning</cite>. This paper provides a method to learn differentiable
            functions expressed
            as programs in a domain-specific language by relaxing programs into differentiable neural networks. Our
            notes can be found
            <a
                href="reading-group-notes/Learning Differentiable Programs with Admissible N 7800a4696e514f559bc4258bb573b807.html">here</a>.
        </div>

    </div>



    <script>
        processDocument();





    </script>

    <div class="footnotes">

        <footnotes>

        </footnotes>

    </div>
    </div>

</body>



</html>