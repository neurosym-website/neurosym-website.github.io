[

  
  {
    "name": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
    "blurb": " <cite>grand2023lilo</cite>",
    "authors": "Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas",
    "link": "https://arxiv.org/abs/2310.19791"
  },


  
  {
    "name": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
    "blurb": " We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. <cite>wong2023word-ours</cite>",
    "authors": "Lionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka, Jacob Andreas, Joshua B. Tenenbaum",
    "link": "https://github.com/gabegrand/world-models"
  },




  {
    "name": "Data Extraction via Semantic Regular Expression Synthesis",
    "blurb": "We introduce semantic regexes, a generalization of regular expressions that facilitates combined syntactic and semantic reasoning about textual data. <cite>dataext-ours</cite>",
    "authors": "Qiaochu Chen, Arko Banerjee, Çağatay Demiralp, Greg Durrett, Isil Dillig",
    "link": "https://github.com/utopia-group/Smore"
  },



  {
    "name": "ImageEye: Batch Image Processing Using Program Synthesis",
    "blurb": " This project presents a new synthesis-based approach for batch image processing. Unlike existing tools that can only apply global edits to the entire image, our method can apply fine-grained edits to individual objects within the image<cite>imageeye-ours</cite>",
    "authors": "",
    "link": "https://github.com/celestebarnaby/ImageEye"
  },


  {
    "name": "SatLM: Satisfiability-Aided Language Models Using Declarative Prompting",
    "blurb": "  In this project, we propose a new satisfiability-aided language modeling (SatLM) approach for improving the reasoning capabilities of large language models<cite>ye2023satlm-ours</cite>",
    "authors": "",
    "link": "https://github.com/xiye17/sat-lm"
  },


  {
    "name": "FI-ODE: Certified and Robust Forward Invariance in Neural ODEs",
    "blurb": "We study how to certifiably enforce forward invariance properties in neural ODEs. Forward invariance implies that the hidden states of the ODE will stay in a good region, and a robust version would hold even under adversarial perturbations to the input <cite>huang2022fi-ours</cite>",
    "authors": "Yujia Huang, Ivan Dario Jimenez Rodriguez, Huan Zhang, Yuanyuan Shi, Yisong Yue",
    "link": "https://github.com/yjhuangcd/FI-ODE"
  },

  {
    "name": "Policy Optimization with Linear Temporal Logic Constraints",
    "blurb": "A method for undiscounted policy optimization under LTL constraints with a generative model in the presence of unknown dynamics <cite>voloshin2022policy-ours</cite>",
    "authors": "Cameron Voloshin, Hoang Minh Le, Swarat Chaudhuri, Yisong Yue",
    "link": "https://openreview.net/attachment?id=yZcPRIZEwOG&name=supplementary_material"
  },

  {
    "name": "Versatile Offline Imitation from Observations and Examples via Regularized State-Occupancy Matching",
    "blurb": "We propose State Matching Offline DIstribution Correction Estimation (SMODICE), a novel and versatile regression-based offline imitation learning (IL) algorithm derived via state-occupancy matching. <cite>ma2022versatile-ours</cite>",
    "authors": "Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, Osbert Bastani",
    "link": "https://github.com/JasonMa2016/SMODICE#smodice-versatile-offline-imitation-learning-via-state-occupancy-matching"
  },
  {
    "name": "Eventual Discounting Temporal Logic Counterfactual Experience Replay",
    "blurb": " This paper makes two contributions. First, we develop a new value-function based proxy, using a technique we call eventual discounting, under which one can find policies that satisfy the LTL specification with highest achievable probability. Second, we develop a new experience replay method for generating off-policy data from on-policy rollouts via counterfactual reasoning on different ways of satisfying the LTL specification. <cite>voloshin2023eventual-ours</cite>",
    "authors": "Cameron Voloshin, Abhinav Verma, Yisong Yue",
    "link": "https://github.com/clvoloshin/RL-LTL"
  },

  {
    "name": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking ",
    "blurb": "Predicting the binding structure of a small molecule ligand to a protein---a task known as molecular docking---is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. <cite>corso2022diffdock-ours</cite>",
    "authors": "Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi S. Jaakkola",
    "link": "https://github.com/gcorso/DiffDock/"
  },

  {
    "name": "Top-Down Synthesis for Library Learning",
    "blurb": "This project introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). <cite>stitch-ours</cite>",
    "authors": "Matthew Bowers, Theo X. Olausson, Lionel Wong, Gabriel Grand, Joshua B. Tenenbaum, Kevin Ellis, Armando Solar-Lezama",
    "link": "https://github.com/mlb2251/stitch"
  },

  {
    "name": "Geoclidean: Few-Shot Generalization in Euclidean Geometry ",
    "blurb": "We introduce Geoclidean, a domain-specific language for Euclidean geometry, and use it to generate two datasets of geometric concept learning tasks for benchmarking generalization judgements of humans and machines. <cite>hsu2022geoclidean-ours</cite>",
    "authors": "Joy Hsu, Jiajun Wu, Noah Goodman",
    "link": "https://github.com/joyhsu0504/geoclidean_framework#geoclidean-few-shot-generalization-in-euclidean-geometry"
  },

  {
    "name": "Peano: learning formal mathematical reasoning",
    "blurb": " we introduce Peano, a theorem-proving environment where the set of valid actions at any point is finite. We use Peano to formalize introductory algebra problems and axioms, obtaining well-defined search problems. <cite>poesia2023peano-ours</cite>",
    "authors": " Gabriel Poesia and Noah D. Goodman",
    "link": "https://github.com/gpoesia/peano#peano---learning-formal-mathematical-reasoning"
  },

  {
    "name": "Metric Program Synthesis",
    "blurb": "We present a new domain-agnostic synthesis technique for generating programs from input-output examples. Our method, called metric program synthesis, relaxes the well-known observational equivalence idea (used widely in bottom-up enumerative synthesis) into a weaker notion of observational similarity, with the goal of reducing the search space that the synthesizer needs to explore. <cite>FeserDS22metric-ours</cite>",
    "authors": "Feser, John, Isil Dillig, and Armando Solar-Lezama",
    "link": "https://github.com/jfeser/symetric"
  },

  {
    "name": "Web question answering with neurosymbolic program synthesis",
    "blurb":"In this paper, we propose a new technique based on program synthesis for extracting information from webpages. Given a natural language query and a few labeled webpages, our method synthesizes a program that can be used to extract similar types of information from other unlabeled webpages <cite>webq-ours</cite>",
    "authors": "Chen, Qiaochu, Aaron Lamoreaux, Xinyu Wang, Greg Durrett, Osbert Bastani, and Isil Dillig",
    "link": "https://github.com/utopia-group/WebQA"
  },

  {
    "name": "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
    "blurb":" We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems <cite>EllisWNSMHCST21-ours</cite>",
    "authors": "Ellis, Kevin, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum",
    "link": "https://github.com/ellisk42/ec"
  },

  {
    "name": "Neural program generation modulo static analysis",
    "blurb":"ate-of-the-art neural models of source code tend to be evaluated on the generation of individual expressions and lines of code, and commonly fail on long-horizon tasks such as the generation of entire method bodies. We propose to address this deficiency using weak supervision from a static program analyzer. <cite>neurostatistic-ours</cite>",
    "authors": "Mukherjee, Rohan, Yeming Wen, Dipak Chaudhari, Thomas Reps, Swarat Chaudhuri, and Christopher Jermaine",
    "link": "https://github.com/rohanmukh/nsg"
  },

  {
    "name": "Safe Neurosymbolic Learning with Differentiable Symbolic Execution",
    "blurb":"We study the problem of learning worst-case-safe parameters for programs that use neural networks as well as symbolic, human-written code. Such neurosymbolic programs arise in many safety-critical domains. <cite>safe-ours</cite>",
    "authors": "Yang, Chenxi, and Swarat Chaudhuri",
    "link": "https://github.com/chenxi-yang/DSE"
  },

  {
    "name": "Program Synthesis Guided Reinforcement Learning for Partially Observed Environments",
    "blurb":"A key challenge for reinforcement learning is solving long-horizon planning problems. Recent work has leveraged programs to guide reinforcement learning in these settings. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. <cite>yang2021program-ours</cite>",
    "authors": "Yang, Yichen, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, and Martin Rinard",
    "link": "https://github.com/yycdavid/program-synthesis-guided-RL "
  },

  {
    "name": "Neurosymbolic transformers for multi-agent communication",
    "blurb":"We study the problem of inferring communication structures that can solve cooperative multi-agent planning problems while minimizing the amount of communication. We quantify the amount of communication as the maximum degree of the communication graph; this metric captures settings where agents have limited bandwidth. <cite> inala2020neurosymbolic </cite>",
    "authors": "Inala, Jeevana Priya, Yichen Yang, James Paulos, Yewen Pu, Osbert Bastani, Vijay Kumar, Martin Rinard, and Armando Solar-Lezama",
    "link": "https://github.com/jinala/multi-agent-neurosym-transformers"
  },

  {
    "name": "Pragmatic Code Autocomplete",
    "blurb":"In this work, we aim to make programming languages more concise by allowing programmers to utilize a controlled level of ambiguity. <cite>poesia2021pragmatic-ours</cite>",
    "authors": "Poesia, Gabriel, and Noah Goodman",
    "link": "https://github.com/gpoesia/magicomplete"
  },

  {
    "name": "DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates",
    "blurb":"CPU simulators are useful tools for modeling CPU execution behavior. However, they suffer from inaccuracies due to the cost and complexity of setting their fine-grained parameters, such as the latencies of individual instructions. This complexity arises from the expertise required to design benchmarks and measurement frameworks that can precisely measure the values of parameters at such fine granularity. <cite>renda2020difftune-ours</cite>",
    "authors": "Renda, Alex, Yishen Chen, Charith Mendis, and Michael Carbin",
    "link": "https://github.com/ithemal/DiffTune "
  },

  {
    "name": "Unsupervised Learning of Neurosymbolic Encoders",
    "blurb":"We present a framework for the unsupervised learning of neurosymbolic encoders, which are encoders obtained by composing neural networks with symbolic programs from a domain-specific language. Our framework naturally incorporates symbolic expert knowledge into the learning process, which leads to more interpretable and factorized latent representations compared to fully neural encoders. <cite>zhan2021unsupervised-ours</cite>",
    "authors": "Eric Zhan, Jennifer J. Sun, Ann Kennedy, Yisong Yue and Swarat Chaudhuri",
    "link": "https://github.com/ezhan94/neurosymbolic-encoders"
  },

  {
    "name": "Learning Differentiable Programs with Admissible Neural Heuristics",
    "blurb":"We study the problem of learning differentiable functions expressed as programs in a domain-specific language. Such programmatic models can offer benefits such as composability and interpretability; however, learning them requires optimizing over a combinatorial space of program architectures. We frame this optimization problem as a search in a weighted graph whose paths encode top-down derivations of program syntax. <cite>near</cite>",
    "authors": "Ameesh Shah, Eric Zhan, Jennifer Sun, Abhinav Verma, Yisong Yue, Swarat Chaudhuri",
    "link": "https://github.com/trishullab/near"
  },

  {
    "name": " Few-Shot Image Classification: Just Use a Library of Pre-Trained Feature Extractors and a Simple Classifier ",
    "blurb":" Recent papers have suggested that transfer learning can outperform sophisticated meta-learning methods for few-shot image classification. We take this hypothesis to its logical conclusion, and suggest the use of an ensemble of high-quality, pre-trained feature extractors for few-shot image classification. <cite>Chowdhury_2021_ICCV-ours</cite>",
    "authors": "Arkabandhu Chowdhury, Mingchao Jiang, Swarat Chaudhuri, Chris Jermaine",
    "link": "https://github.com/arjish/PreTrainedFullLibrary_FewShot"
  },

  {
    "name": "Independent se (3)-equivariant models for end-to-end rigid protein docking",
    "blurb":"Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding <cite>ganea2021independent-ours</cite>",
    "authors": "Ganea, Octavian-Eugen, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause",
    "link": "https://github.com/octavian-ganea/equidock_public"
  },

  {
    "name": "Torsional geometric generation of molecular 3d conformer ensembles",
    "blurb":"<cite>jing2022torsional-ours</cite>",
    "authors": "Ganea, Octavian, Lagnajit Pattanaik, Connor Coley, Regina Barzilay, Klavs Jensen, William Green, and Tommi Jaakkola",
    "link": "https://github.com/PattanaikL/GeoMol"
  },

  {
    "name": "EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction",
    "blurb":"Predicting how a drug-like molecule binds to a specific protein target is a core problem in drug discovery.EquiBind, an SE(3)-equivariant geometric deep learning model performing direct-shot prediction of both i) the receptor binding location (blind docking) and ii) the ligand’s bound pose and orientation. <cite>stark2022equibind-ours</cite>",
    "authors": "Stärk, Hannes, Octavian Ganea, Lagnajit Pattanaik, Regina Barzilay, and Tommi Jaakkola",
    "link": "https://github.com/HannesStark/EquiBind"
  },

  {
    "name": "Torsional Diffusion for Molecular Conformer Generation",
    "blurb":"Molecular conformer generation is a fundamental task in computational chemistry. Several machine learning approaches have been developed, but none have outperformed state-of-the-art cheminformatics methods. We propose torsional diffusion, a novel diffusion framework that operates on the space of torsion angles via a diffusion process on the hypertorus and an extrinsic-to-intrinsic score model. <cite>jing2022torsional-ours</cite>",
    "authors": "Jing, Bowen, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola",
    "link": "https://github.com/gcorso/torsional-diffusion"
  },

  {
    "name": "Crystal diffusion variational autoencoder for periodic material generation",
    "blurb":"Generating the periodic structure of stable materials is a long-standing challenge for the material design community.  We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability  <cite>xie2021crystal-ours</cite>",
    "authors": "Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, Tommi Jaakkola",
    "link": "https://github.com/txie-93/cdvae "
  },

  {
    "name": "Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem",
    "blurb":"<cite>trippe2022diffusion-ours</cite>",
    "authors": "Trippe, Brian L., Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, and Tommi Jaakkola",
    "link": "https://openreview.net/attachment?id=6TxBxqNME1Y&name=supplementary_material"
  },

  {
    "name": "Synthesizing theories of human language with Bayesian program induction",
    "blurb":"Automated, data-driven construction and evaluation of scientific models and theories is a long-standing challenge in artificial intelligence. We present a framework for algorithmically synthesizing models of a basic part of human language: morpho-phonology, the system that builds word forms from sounds <cite>Ellis22Linguistics-ours</cite>",
    "authors": "Ellis, Kevin, Adam Albright, Armando Solar-Lezama, Joshua B. Tenenbaum, and Timothy J. O’Donnell",
    "link": "https://github.com/ellisk42/bpl_phonology"
  },

  {
    "name": "Task programming: Learning data efficient behavior representations",
    "blurb":"we present TREBA: a method to learn annotation-sample efficient trajectory embedding for behavior analysis, based on multi-task self-supervised learning.  <cite>sun2021task-ours</cite>",
    "authors": "Jennifer J. Sun, Ann Kennedy, Eric Zhan, David J. Anderson, Yisong Yue, Pietro Perona",
    "link": "https://github.com/neuroethology/TREBA"
  },

  {
    "name": "Contrastive Reinforcement Learning of Symbolic Reasoning Domains",
    "blurb":"Abstract symbolic reasoning, as required in domains such as mathematics and logic, is a key component of human intelligence. Solvers for these domains have important applications, especially to computer-assisted education. But learning to solve symbolic problems is challenging for machine learning algorithms. Existing models either learn from human solutions or use hand-engineered features, making them expensive to apply in new domains. <cite>poesia2021contrastive-ours</cite>",
    "authors": "Gabriel Poesia, WenXin Dong, Noah Goodman",
    "link": "https://github.com/gpoesia/socratic-tutor"
  },

  {
    "name": "Leveraging Language to Learn Program Abstractions and Search Heuristics",
    "blurb":"We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. <cite>wong2021leveraging-ours</cite>",
    "authors": "Wong, Catherine, Kevin M. Ellis, Joshua Tenenbaum, and Jacob Andreas",
    "link": "https://github.com/ellisk42/ec/tree/icml_2021_supplement"
  },

  {
    "name": " Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis",
    "blurb":" We present AutoSWAP: a framework for automatically synthesizing data-efficient task-level LFs. The key to our approach is to efficiently represent expert knowledge in a reusable domain-specific language. <cite>tseng2022automatic-ours</cite>",
    "authors": "Tseng, Albert, Jennifer J. Sun, and Yisong Yue",
    "link": "https://github.com/tsengalb99/AutoSWAP"
  },

  {
    "name": " Self-Supervised Keypoint Discovery in Behavioral Videos ",
    "blurb":" We propose a method for learning the posture and structure of agents from unlabelled behavioral videos. Starting from the observation that behaving agents are generally the main sources of movement in behavioral videos, our method, Behavioral Keypoint Discovery (B-KinD), uses an encoder-decoder architecture with a geometric bottleneck to reconstruct the spatiotemporal difference between video frames. <cite>sun2022self-ours</cite>",
    "authors": "Sun, Jennifer J., Serim Ryou, Roni H. Goldshmid, Brandon Weissbourd, John O. Dabiri, David J. Anderson, Ann Kennedy, Yisong Yue, and Pietro Perona.",
    "link": "https://github.com/neuroethology/BKinD"
  },

  {
    "name": "The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions",
    "blurb":"Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay <cite>sun2021multi-ours</cite>",
    "authors": "Jennifer J. Sun, Tomomi Karigo, Dipam Chakraborty, Sharada P. Mohanty, Benjamin Wild, Quan Sun, Chen Chen, David J. Anderson, Pietro Perona, Yisong Yue, Ann Kennedy",
    "link": "https://data.caltech.edu/records/s0vdx-0k302 "
  },

  {
    "name": "The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior",
    "blurb":"We introducing a large-scale, multi-agent trajectory dataset from real-world behavioral neuroscience experiments that covers a range of behavior analysis tasks. Our dataset consists of trajectory data from common model organisms, with 9.6 million frames of mouse data and 4.4 million frames of fly data, in a variety of experimental settings, such as different strains, lengths of interaction, and optogenetic stimulation. <cite>sun2022mabe22-ours</cite>",
    "authors": "Jennifer J. Sun, Andrew Ulmer, Dipam Chakraborty, Brian Geuther, Edward Hayes, Heng Jia, Vivek Kumar, Zachary Partridge, Alice Robie, Catherine E. Schretter, Chao Sun, Keith Sheppard, Param Uttarwar, Pietro Perona, Yisong Yue, Kristin Branson, Ann Kennedy",
    "link": "https://data.caltech.edu/records/8kdn3-95j37"
  },

  {
    "name": "Leveraging Language to Learn Program Abstractions and Search Heuristics",
    "blurb": "We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. <cite>wong2021leveraging-ours</cite>",
    "authors": "Catherine Wong, Kevin Ellis, Joshua B. Tenenbaum, Jacob Andreas",
    "link": "https://github.com/CatherineWong/laps_dreamcoder"
  },
  {
    "name": "Programming with neural surrogates of programs",
    "blurb": " Surrogates, models that mimic the behavior of programs, form the basis of a variety of development workflows. We study three surrogate-based design patterns, evaluating each in case studies on a large-scale CPU simulator. <cite>renda2021programming-ours</cite>",
    "authors": "Alex Renda, Yi Ding, Michael Carbin",
    "link": "https://github.com/psg-mit/programming-with-neural-surrogates-of-programs"
  }
]
